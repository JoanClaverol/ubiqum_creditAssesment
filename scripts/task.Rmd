---
title: "Data science with python"
author: "Joan Claverol - Data analyst"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
if (require(pacman) == FALSE) {
    install.packages("pacman")
}
pacman::p_load(reticulate, dplyr)
```

## Define the goals

We have to focus the **BADIR** perspective; 

* **B** *usiness question*
    - What is the stated business question?
    - What is the intent underlying the question (e.g., what is the context, what is the impacted segment, and what are stakeholders’ current thoughts about the underlying reasons?
    - What business considerations (e.g., stakeholders, timeline, and cost) are likely to impact the analysis?

* **A** *nalysis plan*
    - What is the analysis goal?
    - What hypotheses are to be tested?
    - What data is required/available to test the hypotheses?
    - What methodology(-ies) will you employ?
    - What is the project plan (timeline and milestones, risks, phasing, prioritization, …)?

* **D** *ata collection*
    - From where can the data be obtained?
    - How must the data be cleansed and validated?

* **I** *nsights*
    - What patterns do you see in the data?
    - Are each of the hypotheses proven or disproven?
    - How much confidence should stakeholders place in the results?
    - How do you rank your findings in terms of quantified impact on the business?

* **R** *ecommendation*
    - How can you most effectively present the results of your analysis to your stakeholders (in terms they can understand and in alignment with information they’ll value)?
    - Note: A generic template for a recommendation presentation or report might include:
    - Objective: Background (optional), Scope (optional), Approach (optional), Recommendations, Key insights with impact, Next steps
    
## Prepare and explore the data

Problem:
1. Increase in customer default rates - This is bad for Credit One since we approve the customers for loans in the first place.
2. Revenue and customer loss for clients and, eventually, loss of clients for Credit One

Investigative Questions:
1. How do you ensure that customers can/will pay their loans? Can we do this?

As you progress through the tasks at hand begin thinking about how to solve this problem. Here are some lessons we learned form a similar problem we addressed last year:
1. We cannot control customer spending habits
2. We cannot always go from what we find in our analysis to the underlying "why"
3. We must on the problem(s) we can solve: What attributes in the data can we deem to be statistically significant to the problem at hand?
4. What concrete information can we derive from the data we have?
5. What proven methods can we use to uncover more information and why?

###### Libraries

```{python}
# import libraries to use
import pandas as pd
import numpy as np
from math import sqrt
import matplotlib.pyplot as plt
```

### Import the data we are going to explore

```{python}
data = pd.read_csv("data/default of credit card clients.csv", header=1)
```

Main functions to use for explore the data: 

```{python}
data.columns.values
```

Or do a quick exploration of the data: 

```{python}
data.describe()
```

And find the data types: 

```{python}
data.dtypes
# we will have to make some data changes
```

Get the information of your data: 

```{python}
data.info()
```


We will have to make changes to our data to adapt it to the right format:

* Transform to characters *sex, marriage and education*

Steps to follow to clean the data: 

* Data cleaning
* Data transformation
    - How to deal with missing values?
* Data reduction
* Data discretization
* Text cleaning (if needed)

### Data cleaning

We will use pandas library to perform this task

Functions like `pd.notnull(data)` or `pd.isnull(data)` to find missing values:

```{python}
pd.isnull(data).values.ravel().sum()
```

There is no missing values. So we can skip to use functions like: 

* `data.dropna(axis=0, how = "all")`, to take out all NAs, or you can specify other options in its parameters

### Data visualization 

Exploratory Data Analysis (EDA) is basic to be able to get insights from the data: 

* Visualization and Statistics about each variable - you've already done part of this with describe(). 
* Scatter plots comparing the relationships between any two variables
* If needed, visualizing the relationship between many (more than 2) variables
* Checking for collinearity or performing dimensionality reduction 

```{python}
# import python libraries
import matplotlib.pyplot as plt

# histogram 
plt.hist(data["LIMIT_BAL"], bins= 50)
plt.show()
```

## Build and evaluate the models 

Now that you have properly prepared and thoroughly explored the data it's time to begin the modeling process. Throughout this task will examine feature selection and model building through the use of the Python module called Sci-Kit Learn. Is very important for you to understand that this task uses the CreditOne data in a regression type problem, but your final analysis will be centered on classification. The steps will be very similar, but you will need to replicate and them in a different way and obviously on different features and variables. Let's get started with an introduction to Sci-Kit Learn and how it differs from what you've already done with caret and R.

##### Libraries machine learning

```{python}
# estimators
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn import linear_model

# model metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score

# problems with the PA, there is a renaming in packages: 
# https://stackoverflow.com/questions/30667525/importerror-no-module-named-sklearn-cross-validation
# https://stackoverflow.com/questions/40704484/importerror-no-module-named-model-selection
# cross validation
from sklearn.model_selection import train_test_split
```

### Moving inside a dataframe

We can select a subset of data inside a data frame in pandas:

```{python}
variable_name = data.iloc[1:3,2:4]
variable_name
```

You can also select a range of columns by specify: 

```{python}
data.iloc[:,0:4].head()
```

#### 1. Select the features

```{python}
#features
features = data.iloc[:,12:23]
print('Summary of feature sample')
features.columns.values
```

#### 2. Select the dependent variables 

```{python}
# dependent variable
depVar = data["PAY_AMT6"]
```

#### 3. Establish the training set for the X-variables or Feature space (first 1000 rows: only for this example you will still follow a 70/30 split for your final models)

```{python}
# Training set (feature space: X training) 
x_train = features[: 1000]
x_train.head()
```

#### 4. Establish the training set for the Y-variable or dependent variable (the number of rows much match the X-training set)

```{python}
#Dependent Variable Training Set (y Training)
y_train = 2
y_train_count = len(y_train.index)
print('The number of observations in the Y training set are:',str(y_train_count))
y_train.head()
```

#### 5. Establish the testing set for the X-Variables or Feature space

```{python}
#Testing Set (X Testing)
x_test = features[-100:]
x_test_count = len(X_test.index)
print('The number of observations in the feature testing set is:',str(X_test_count))
print(X_test.head())
```

#### 6. Establish Ground truth 

```{python}
#Ground Truth (y_test) 
y_test = depVar[:-100]
y_test_count = len(y_test.index)
print('The number of observations in the Y training set are:',str(y_test_count))
y_test.head()
```

